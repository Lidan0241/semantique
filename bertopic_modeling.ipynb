{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6661e8",
   "metadata": {},
   "source": [
    "# Topic modeling with BERTopic\n",
    "\n",
    "BERTopic est un modèle de sujet qui exploite les techniques de clustering et une variante de TF-IDF basée sur les classes pour la génération des sujets :\n",
    "1. création des document embeddings à l'aide d'un modèle de langage pré-entraîné \n",
    "2. réduction de la dimensionnalité des embeddings des documents avant de créer des clusters de documents sémantiquement similaires, chacun représentant un sujet distinct\n",
    "3. application d'une version de TF-IDF basée sur les classes afin d'extraire la représentation des sujets (*topic representation*) de chaque sujet.\n",
    "\n",
    "*Topic* est un cluster de documents sémantiquement similaires\n",
    "*Topic representation* est la façon dont les sujets sont décrits ou labélisés, ici par un set de mots extraits des documents du cluster.\n",
    "\n",
    "=> https://medialab.sciencespo.fr/actu/les-enjeux-de-linformation-a-lere-numerique-vus-par-les-francais/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd62118d-af23-4823-aba4-1b5d2bdcdc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nw/mys85hsx6fgdrrqyzscv96100000gn/T/ipykernel_68332/535357084.py\", line 3, in <module>\n",
      "    from spacy.lang.fr.stop_words import STOP_WORDS\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/thinc/types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/lydia/miniconda3/envs/semantique/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lang.zh.stop_words import STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous avons besoin de `protobuf` version 3.20 pour `SentenceTransformer` avec camembert\n",
    "# !pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccb4de",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "Les modèles de sujet avec transformers et word-embedding ne demande, en théorie, aucun prétraitement des données. Cependant, il peut s'avérer utile de normaliser les données, d'enlever les balises HTML (quand ce n'est pas déjà fait), etc. Dans le cas de certains corpus, la suppression de mots spécifiques présents dans tout le corpus peut également s'avérer utile. Après cela, on peut transformer notre dataframe en list de documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe952b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): corpus.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpus.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m      2\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m )\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m      4\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mis_not_null()\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mget_column(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mto_list() \u001b[38;5;66;03m# remplacer captions par la colonne contenant vos publications\u001b[39;00m\n\u001b[1;32m      8\u001b[0m docs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/semantique/lib/python3.10/site-packages/polars/_utils/deprecation.py:114\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    111\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    112\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantique/lib/python3.10/site-packages/polars/_utils/deprecation.py:114\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    111\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    112\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantique/lib/python3.10/site-packages/polars/_utils/deprecation.py:114\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    111\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    112\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantique/lib/python3.10/site-packages/polars/io/csv/functions.py:539\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[1;32m    533\u001b[0m         source,\n\u001b[1;32m    534\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    538\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m--> 539\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8-lossy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "File \u001b[0;32m~/miniconda3/envs/semantique/lib/python3.10/site-packages/polars/io/csv/functions.py:687\u001b[0m, in \u001b[0;36m_read_csv_impl\u001b[0;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    685\u001b[0m projection, columns \u001b[38;5;241m=\u001b[39m parse_columns_arg(columns)\n\u001b[0;32m--> 687\u001b[0m pydf \u001b[38;5;241m=\u001b[39m \u001b[43mPyDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory (os error 2): corpus.csv"
     ]
    }
   ],
   "source": [
    "docs = pl.read_csv(\"corpus.csv\").with_columns(\n",
    "    pl.col(\"captions\").str.normalize(\"NFC\")\n",
    ").filter(\n",
    "    pl.col(\"captions\").is_not_null()\n",
    ").get_column(\n",
    "    \"captions\"\n",
    ").to_list() # remplacer captions par la colonne contenant vos publications\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3123d3-f25b-4a4b-b6e4-9a281758bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499a96",
   "metadata": {},
   "source": [
    "Après cela, nous préparons une liste de mots vides, que nous exclurons de la représentation des sujets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2abbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoplist = list(STOP_WORDS)\n",
    "# ADDITIONAL_STOPWORDS = []\n",
    "# stoplist.extend(ADDITIONAL_STOPWORDS)\n",
    "stoplist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98236674",
   "metadata": {},
   "source": [
    "# Vectoriser les données\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "La première étape consiste à transformer une chaîne (phrase) en un tableau de nombres (vecteur), autrement dit à vectoriser le texte.\n",
    "BERTopic peut convertir les documents en embeddings. Cependant, ce processus peut être très coûteux. Il est donc possible de calculer ces embeddings une seule fois et de les transmettre à BERTopic pour éviter de les calculer à chaque fois.\n",
    "\n",
    "=> https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "=> https://huggingface.co/spaces/mteb/leaderboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17030859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\"ah ouais c'est ça votre argument ultime\", \"aujourd'hui ils sont contraignants\"]\n",
    "\n",
    "embedding_model = SentenceTransformer(\"dangvantuan/sentence-camembert-base\") # (\"all-MiniLM-L6-v2\")\n",
    "example_embeddings = embedding_model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "print(f\"Sentence 1 has {len(sentences[0])} characters, and the embedding is {len(example_embeddings[0])} long.\")\n",
    "print(f\"Sentence 2 has {len(sentences[1])} characters, and the embedding is {len(example_embeddings[1])} long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b35d2",
   "metadata": {},
   "source": [
    "Comme le montre la cellule ci-dessus, les embeddings continues créées par le transformateur de phrases `dangvantuan/sentence-camembert-base` ont la même longueur, malgré des phrases de longueurs différentes. Ces embeddings continues ont la même longueur car, plutôt que de représenter directement les mots sous forme de nombres (c'est-à-dire de « sac de mots »), le transformateur crée une représentation riche prenant en compte 768 dimensions.\n",
    "\n",
    "=> https://github.com/MaartenGr/BERTopic/discussions/822"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968392b0-d064-416c-a1d0-78daa67d8b48",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "BERTopic utilise généralement un algorithme de réduction de dimensionnalité (*dimensionality reduction algorithm*) pour réduire la taille des embeddings. Ceci permet d'éviter, dans une certaine mesure, le fléau de la dimension (https://en.wikipedia.org/wiki/Curse_of_dimensionality). \n",
    "\n",
    "UMAP est utilisé pour réduire l'espace dimensionnel. Cependant, il affiche par défaut un comportement stochastique qui produit des résultats différents à chaque exécution. Pour éviter cela pendant la période de test, nous pouvons définir un random_state du modèle avant de le transmettre à BERTopic.\n",
    "\n",
    "```python\n",
    "UMAP(angular_rp_forest=True, metric='cosine', n_components=10, n_neighbors=30, min_dist=0.1, random_state=42)\n",
    "```\n",
    "\n",
    "=> https://umap-learn.readthedocs.io/en/latest/parameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d4d07-095d-4e3d-9569-2333a3a2cf64",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "\n",
    "`nr_topics` est le paramètre qui permet de contrôler le nombre de sujets en fusionnant les sujets après leur création. Il permet de créer un nombre fixe de sujets. Il est toutefois conseillé de contrôler le nombre de sujets via le modèle de cluster, HDBSCAN par défaut. HDBSCAN possède un paramètre, `min_cluster_size`, qui contrôle indirectement le nombre de sujets créés. Un `min_cluster_size` élevé génère moins de sujets, tandis qu'un `min_cluster_size` faible en génère davantage.\n",
    "\n",
    "```python\n",
    "HDBSCAN(min_cluster_size=13, min_samples=3, prediction_data=True, metric='euclidean', cluster_selection_method='eom')\n",
    "```\n",
    "\n",
    "=> https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c74832-5ece-4c5c-8b52-0cffe1ff7ffd",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "La représentation par défaut des sujets est calculée via c-TF-IDF. Cependant, c-TF-IDF s'appuie sur CountVectorizer, qui convertit le texte en unités. Grâce à CountVectorizer, nous pouvons effectuer plusieurs opérations : supprimer les mots vides, ignorer les mots rares, augmenter la range des n-gram. En d'autres termes, nous pouvons prétraiter les représentations des sujets après l'attribution des documents aux sujets. Cela n'aura aucune incidence sur le processus de clustering.\n",
    "\n",
    "La tokenisation se fait automatiquement au niveau des espaces, pour le chinois il faut rajouter :\n",
    "\n",
    "```python\n",
    "import jieba\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f185f96",
   "metadata": {},
   "source": [
    "### Construction du modèle\n",
    "\n",
    "=> https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90258ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_model_parameters(embedding_model):\n",
    "\n",
    "    # Step 1 - extraction des embeddings\n",
    "    embedding_model = embedding_model\n",
    "\n",
    "    # Step 2 - réduction des dimensionalité\n",
    "    umap_model = UMAP(angular_rp_forest=True, metric='cosine', n_components=10, n_neighbors=30, min_dist=0.1)\n",
    "\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=5, prediction_data=True, metric='euclidean', cluster_selection_method='eom')\n",
    "    # hdbscan_model = KMeans(n_clusters=50)\n",
    "    # hdbscan_model = sklearn.cluster.AgglomerativeClustering(n_clusters=50)\n",
    "\n",
    "    # Step 4 - Tokenisation des topics\n",
    "\n",
    "    stoplist = list(STOP_WORDS)\n",
    "    ADDITIONAL_STOPWORDS = [\"qu\", \"ya\", \"faut\", \"euh\"]\n",
    "    stoplist.extend(ADDITIONAL_STOPWORDS)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=stoplist, ngram_range=(1, 3), max_df=0.5)\n",
    "\n",
    "    # Step 5 - Création de la représentation des topics\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "\n",
    "    # Topic model\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        language='french',\n",
    "        n_gram_range=(1,3),\n",
    "        nr_topics=\"auto\"\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00dd83",
   "metadata": {},
   "source": [
    "# Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2be5a1",
   "metadata": {},
   "source": [
    "On encode tous les documents de notre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b5c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"dangvantuan/sentence-camembert-base\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196a5c",
   "metadata": {},
   "source": [
    "On crée une instance du modèle avec tous les paramètres définis et notre modèle d'embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d605d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model = set_model_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f4816",
   "metadata": {},
   "source": [
    "On ajuste le modèle aux documents de notre corpus et aux embeddings que nous avons préparés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773525a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.fit(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df908c8-d03c-43ae-988e-41b572d87400",
   "metadata": {},
   "source": [
    "-1 est un \"faux\" cluster contenant toutes les valeurs aberrantes (*outliers*) ; il ne faut donc pas le prendre en compte. Un nom représentatif est attribué à chaque sujet. Le nom par défaut est un identifiant numérique suivi des mots-clés les plus représentatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b275aa6-6d95-4c58-a2c8-8e7e6c83b384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f100d3c-6b71-4760-9214-a976fcd12e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5842f10-5856-48e9-8e29-c0e2f678752c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.generate_topic_labels(nr_words=20, topic_prefix=True, word_length=None, separator='  --  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f64ad4-cd34-48e5-a02e-017333e9c1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.save('last_bertopic.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c10732",
   "metadata": {},
   "source": [
    "# Exploration des prédictions du modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a28bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load('last_bertopic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0ba77-cc65-43f7-88cc-75b9059dd6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topics_to_merge = [[3,4,6],[3,15]]\n",
    "# topic_model.merge_topics(docs, topics_to_merge)\n",
    "\n",
    "barchart = topic_model.visualize_barchart(top_n_topics=17, title=\"Représentation des topics\", width=400, n_words=7)\n",
    "barchart.write_html('barchart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3befc9-70fe-4d35-a0b0-f21987627ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_labels_dict = {\n",
    "    0:\"L'opinion & le journalisme\",\n",
    "    1:\"Financement & l'indépendance des médias\",\n",
    "    2:\"Désinformation\"\n",
    "}\n",
    "\n",
    "topic_model.set_topic_labels(topic_labels_dict)\n",
    "barchart = topic_model.visualize_barchart(top_n_topics=17, custom_labels=True, title=\"Représentation des topics<\", width=800, n_words=7)\n",
    "barchart.write_html('barchart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d163e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267f2cd-76ff-4ee9-8bbf-d776557e935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "hierarchy = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics, width=1200) # custom_labels=topic_labels_dict\n",
    "hierarchy.write_html('hierarchy.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ace67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap = topic_model.visualize_heatmap(n_clusters=3, width=1200) # custom_labels=topic_labels_dict\n",
    "heatmap.write_html('heatmap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cbb91-9f46-427c-9c07-b6476a312855",
   "metadata": {},
   "source": [
    "Visualisation du classement de tous les termes sur tous les sujets. Chaque sujet est représenté par un ensemble de mots. Cependant, ces mots ne représentent pas tous le sujet de manière égale. Cette visualisation montre combien de mots sont nécessaires pour représenter un sujet et à partir de quel seuil l'effet bénéfique de l'ajout de mots commence à diminuer.\n",
    "\n",
    "Il ya deux axes :\n",
    "- x : Représente le rang des termes dans chaque sujet. Le terme avec le score c-TF-IDF le plus élevé a un rang de 1, le deuxième terme a un rang de 2, et ainsi de suite.\n",
    "- y : Montre le score c-TF-IDF des termes. Plus le score est élevé, plus le terme est représentatif du sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf41be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank(log_scale=True).write_html(\"term_rank.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde642d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().write_html(\"visualize_topics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f5b55",
   "metadata": {},
   "source": [
    "Enregistrer les résultats dans un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6020b-d3c2-4b8e-a7a0-e5f39985e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FILE = 'bertopic_topics_notebook.csv'\n",
    "\n",
    "results = topic_model.get_document_info(docs=docs)\n",
    "# results.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158e097-e696-4583-b23b-ac074d9db5c7",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "LSA commence par créer une matrice terme-document. Les valeurs de la matrice sont généralement des scores TF-IDF qui pondèrent l'importance des mots dans chaque document. La matrice est ensuite décomposée en trois matrices à l'aide de la SVD. Ces matrices capturent les relations entre les termes et les documents. Enfin, en sélectionnant uniquement les premières composantes singulières, LSA réduit la dimensionalité tout en conservant les informations les plus importantes, ce qui permet de découvrir les sujets sous-jacents.\n",
    "\n",
    "```python\n",
    "embedding_model = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.feature_extraction.text.CountVectorizer(analyzer=\"word\", tokenizer=lambda x: x.split(\" \")),\n",
    "    sklearn.decomposition.TruncatedSVD(10)\n",
    ")\n",
    "```\n",
    "\n",
    "=> https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf771a8-fb86-44c0-9b1d-44754c3735f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_parameters_lsa():\n",
    "\n",
    "    # Step 1 - extraction des embeddings\n",
    "    embedding_model = sklearn.pipeline.make_pipeline(\n",
    "        sklearn.feature_extraction.text.CountVectorizer(analyzer=\"word\", tokenizer=lambda x: x.split(\" \")),\n",
    "        sklearn.decomposition.TruncatedSVD(n_components=15)\n",
    "    )\n",
    "\n",
    "    # Step 2 - réduction des dimensionalité\n",
    "    umap_model = UMAP(angular_rp_forest=True, metric='cosine', n_components=10, n_neighbors=20, min_dist=0.1)\n",
    "\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=5, prediction_data=True, metric='euclidean', cluster_selection_method='eom')\n",
    "    # hdbscan_model = KMeans(n_clusters=50)\n",
    "    # hdbscan_model = sklearn.cluster.AgglomerativeClustering(n_clusters=50)\n",
    "\n",
    "    # Step 4 - Tokenisation des topics\n",
    "    stoplist = list(STOP_WORDS)\n",
    "    ADDITIONAL_STOPWORDS = [\"qu\", \"ya\", \"faut\", \"euh\"]\n",
    "    stoplist.extend(ADDITIONAL_STOPWORDS)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=stoplist, ngram_range=(1, 2))\n",
    "\n",
    "    # Step 5 - Création de la représentation des topics\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "\n",
    "    # Topic model\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        language='french',\n",
    "        n_gram_range=(1,2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd77b5-320b-4683-a4e6-8509beffc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = set_model_parameters_lsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cecf4-1db7-4262-8cc5-a70dfdb37369",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c1964-68b6-4106-bfba-08a4db4bf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.generate_topic_labels(nr_words=20, topic_prefix=True, word_length=None, separator='  --  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dfd3f-a98e-4ed3-8679-46a9211db3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a336c-f175-4db8-a334-d3f6ac0dca11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantique",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
